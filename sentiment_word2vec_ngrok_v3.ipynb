{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377b94e5",
   "metadata": {},
   "source": [
    "# Sentiment Analysis — Word2Vec (Google News) + TensorFlow + Ngrok\n",
    "\n",
    "**This notebook uses the pretrained Google News Word2Vec (300-dim)**, builds a Keras model that uses those embeddings, visualizes dataset splits and training curves, then launches a modern Tailwind-styled chatbox served by Flask and exposed with pyngrok. \n",
    "\n",
    "**Important:** The Google News Word2Vec model (~1.6GB) will be downloaded via `gensim.downloader`. Be sure you have enough disk and a stable connection in Colab.\n",
    "\n",
    "Run the cells sequentially (or **Run all**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6bd87",
   "metadata": {},
   "source": [
    "## 1 — Install & imports\n",
    "\n",
    "Installs: `gensim` (for pretrained word2vec), `pyngrok` (tunnel), and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow tensorflow-datasets gensim==4.4.0 pyngrok matplotlib scikit-learn\n",
    "\n",
    "# Imports\n",
    "import os, re, pickle, json, time\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim import downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('TensorFlow', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28eeb86",
   "metadata": {},
   "source": [
    "## 2 — Load IMDB dataset (tfds) and quick inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True)\n",
    "train_texts = []; train_labels = []\n",
    "for t,l in tfds.as_numpy(ds_train):\n",
    "    train_texts.append(t.decode('utf-8')); train_labels.append(int(l))\n",
    "test_texts = []; test_labels = []\n",
    "for t,l in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(t.decode('utf-8')); test_labels.append(int(l))\n",
    "\n",
    "print('Train samples:', len(train_texts), 'Test samples:', len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ca263",
   "metadata": {},
   "source": [
    "## 3 — Preprocess (clean) and visualize dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeee8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"<.*?>\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\s']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "train_clean = [clean_text(t) for t in train_texts]\n",
    "test_clean = [clean_text(t) for t in test_texts]\n",
    "\n",
    "# Show sample lengths distribution and label distribution\n",
    "all_labels = np.array(train_labels + test_labels)\n",
    "labels, counts = np.unique(all_labels, return_counts=True)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(['neg','pos'], counts)\n",
    "plt.title('Label distribution (train+test)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# text length histogram (tokens)\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "lengths = [len(text_to_word_sequence(t)) for t in train_clean]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(lengths, bins=40)\n",
    "plt.title('Train text token length distribution')\n",
    "plt.xlabel('Tokens'); plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8dbc00",
   "metadata": {},
   "source": [
    "## 4 — Tokenize, split and prepare sequences\n",
    "\n",
    "We use a fixed `VOCAB_SIZE` and `MAXLEN`. Tokenizer is fit on train only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "MAXLEN = 200\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_clean)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_clean)\n",
    "X = pad_sequences(X, maxlen=MAXLEN, padding='post', truncating='post')\n",
    "y = np.array(train_labels)\n",
    "\n",
    "# Further split train -> train/val for training\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "print('Shapes:', X_train.shape, X_val.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a4605c",
   "metadata": {},
   "source": [
    "## 5 — Download pretrained Word2Vec (Google News, 300-dim)\n",
    "\n",
    "This downloads the `word2vec-google-news-300` model via `gensim`.\n",
    "**Note:** ~1.6GB download — may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download via gensim-data (this may take a while)\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')  # KeyedVectors\n",
    "print('Loaded pretrained vectors. Vocab size:', len(wv.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3c145",
   "metadata": {},
   "source": [
    "## 6 — Build embedding matrix mapping tokenizer -> pretrained vectors\n",
    "\n",
    "Words not found in pretrained vectors get small random vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ae57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
    "embedding_matrix = np.random.normal(size=(num_words, EMBEDDING_DIM)).astype(np.float32) * 0.01\n",
    "\n",
    "found = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words: continue\n",
    "    if word in wv:\n",
    "        embedding_matrix[i] = wv[word]\n",
    "        found += 1\n",
    "print('Embedding matrix shape:', embedding_matrix.shape, 'Found pretrained vectors for', found, 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e21e8b",
   "metadata": {},
   "source": [
    "## 7 — Build model (Embedding with pretrained weights -> BiLSTM -> Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, GlobalMaxPool1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def build_model(num_words=num_words, embed_dim=EMBEDDING_DIM, embedding_matrix=embedding_matrix):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=num_words, output_dim=embed_dim, weights=[embedding_matrix], input_length=MAXLEN, trainable=False, name='pretrained_embed'),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687aaad",
   "metadata": {},
   "source": [
    "## 8 — Train model (with ModelCheckpoint & EarlyStopping)\n",
    "\n",
    "We record history to plot accuracy and loss curves later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e06cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'sentiment_best.keras'\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=8, batch_size=128, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b1470",
   "metadata": {},
   "source": [
    "## 9 — Plot training & validation accuracy / loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ca54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss\n",
    "hist = history.history\n",
    "epochs = range(1, len(hist['loss'])+1)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, hist['accuracy'], label='train_acc')\n",
    "plt.plot(epochs, hist['val_accuracy'], label='val_acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch'); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, hist['loss'], label='train_loss')\n",
    "plt.plot(epochs, hist['val_loss'], label='val_loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch'); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526f133",
   "metadata": {},
   "source": [
    "## 10 — Evaluate on Test Set (IMDB test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd19d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test sequences using tokenizer\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_clean), maxlen=MAXLEN, padding='post')\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Load best model and evaluate\n",
    "best = tf.keras.models.load_model('sentiment_best.keras')\n",
    "loss, acc = best.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474344e",
   "metadata": {},
   "source": [
    "## 11 — Save model (.keras) and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4409a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'sentiment_w2v_model.keras'\n",
    "best.save(model_save_path, include_optimizer=False)\n",
    "with open('tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print('Saved model and tokenizer:', model_save_path, 'tokenizer.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1493fcc",
   "metadata": {},
   "source": [
    "## 12 — Load saved model and test single predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269260f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.keras.models.load_model('sentiment_w2v_model.keras')\n",
    "with open('tokenizer.pickle', 'rb') as f: tk = pickle.load(f)\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    t = clean_text(text)\n",
    "    seq = pad_sequences(tk.texts_to_sequences([t]), maxlen=MAXLEN, padding='post')\n",
    "    prob = float(loaded.predict(seq)[0][0])\n",
    "    return prob\n",
    "\n",
    "for s in ['I loved this movie', 'I hated this film', 'It was okay, not great']:\n",
    "    print(s, '->', predict_sentiment(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d752ed",
   "metadata": {},
   "source": [
    "## 13 — Flask app with Tailwind chat UI + pyngrok\n",
    "\n",
    "This cell starts a Flask app and exposes it via `pyngrok`. The chat UI uses Tailwind CDN for styling. When you run this cell in Colab it will print the public ngrok URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb19967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask + pyngrok server\n",
    "!pip install -q flask\n",
    "from flask import Flask, request, jsonify, render_template_string\n",
    "from pyngrok import ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = tf.keras.models.load_model('sentiment_w2v_model.keras')\n",
    "with open('tokenizer.pickle', 'rb') as f: tokenizer = pickle.load(f)\n",
    "\n",
    "# Tailwind-styled HTML template (inline)\n",
    "TEMPLATE = '''\n",
    "<!doctype html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\" />\n",
    "<title>Sentiment Chatbox</title>\n",
    "<script src=\"https://cdn.tailwindcss.com\"></script>\n",
    "</head>\n",
    "<body class=\"bg-gray-100 min-h-screen flex items-center justify-center p-4\">\n",
    "  <div class=\"w-full max-w-2xl bg-white rounded-2xl shadow-lg p-6\">\n",
    "    <h2 class=\"text-2xl font-semibold mb-4\">Sentiment Chatbox</h2>\n",
    "    <div id=\"chat\" class=\"h-80 overflow-y-auto border rounded p-3 space-y-3 bg-gray-50\"></div>\n",
    "    <div class=\"flex gap-2 mt-4\">\n",
    "      <input id=\"msg\" class=\"flex-1 border rounded p-2\" placeholder=\"Type a message...\" />\n",
    "      <button onclick=\"send()\" class=\"bg-blue-600 text-white px-4 py-2 rounded\">Send</button>\n",
    "    </div>\n",
    "  </div>\n",
    "<script>\n",
    "async function send(){\n",
    "  const msg = document.getElementById('msg').value;\n",
    "  if(!msg) return;\n",
    "  const chat = document.getElementById('chat');\n",
    "  const userDiv = document.createElement('div');\n",
    "  userDiv.innerHTML = '<div class=\"text-right\"><span class=\"inline-block bg-blue-100 text-blue-800 px-3 py-1 rounded\">'+msg+'</span></div>';\n",
    "  chat.appendChild(userDiv);\n",
    "  chat.scrollTop = chat.scrollHeight;\n",
    "  document.getElementById('msg').value = '';\n",
    "  // send to server\n",
    "  const resp = await fetch('/predict', {\n",
    "    method: 'POST', headers: {'Content-Type':'application/json'}, body: JSON.stringify({text: msg})\n",
    "  });\n",
    "  const data = await resp.json();\n",
    "  const botDiv = document.createElement('div');\n",
    "  const score = parseFloat(data.probability);\n",
    "  const label = score >= 0.5 ? 'Positive' : 'Negative';\n",
    "  botDiv.innerHTML = '<div class=\"text-left\"><span class=\"inline-block bg-gray-100 text-gray-900 px-3 py-1 rounded\">Sentiment: <strong>'+label+'</strong> ('+score.toFixed(4)+')</span></div>';\n",
    "  chat.appendChild(botDiv);\n",
    "  chat.scrollTop = chat.scrollHeight;\n",
    "}\n",
    "</script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string(TEMPLATE)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    text = data.get('text','')\n",
    "    t = clean_text(text)\n",
    "    seq = pad_sequences(tokenizer.texts_to_sequences([t]), maxlen=MAXLEN, padding='post')\n",
    "    prob = float(model.predict(seq)[0][0])\n",
    "    return jsonify({'probability': prob})\n",
    "\n",
    "# Start ngrok tunnel and Flask app\n",
    "public_url = ngrok.connect(5000).public_url\n",
    "print('Ngrok URL:', public_url)\n",
    "app.run(port=5000)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
